1. Perceptron algorithm for binary classification

def linear_regression(X, y, lr=0.01, epochs=1000):
    w, b = 0, 0
    n = len(y)
    for _ in range(epochs):
        y_pred = w * X + b
        dw = (-2/n) * sum(X * (y - y_pred))
        db = (-2/n) * sum(y - y_pred)
        w -= lr * dw
        b -= lr * db
    return w, b

2. Linear Regression using Gradient Descent

def linear_regression(X, y, lr=0.01, epochs=1000):
    w, b = 0, 0
    n = len(y)
    for _ in range(epochs):
        y_pred = w * X + b
        dw = (-2/n) * sum(X * (y - y_pred))
        db = (-2/n) * sum(y - y_pred)
        w -= lr * dw
        b -= lr * db
    return w, b

3. feedforward neural network using Keras.

from keras.models import Sequential
from keras.layers import Dense
model = Sequential()
model.add(Dense(10, input_dim=2, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy',
metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=1)

4. classify images using CNN on MNIST dataset

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(-1, 28, 28, 1) / 255.0

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5)

5. Logistic Regression for binary classification

import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, lr=0.1, epochs=1000):
    w = np.zeros(X.shape[1])
    b = 0
    for _ in range(epochs):
        z = np.dot(X, w) + b
        y_pred = sigmoid(z)
        error = y - y_pred
        w += lr * np.dot(X.T, error)
        b += lr * sum(error)
    return w, b

6. K-Nearest Neighbors (KNN) algorithm for classification

from collections import Counter
import numpy as np

def euclidean(a, b):
    return np.sqrt(np.sum((a - b)**2))

def knn(X_train, y_train, x_test, k=3):
    distances = [euclidean(x_test, x) for x in X_train]
    k_indices = np.argsort(distances)[:k]
    k_labels = [y_train[i] for i in k_indices]
    return Counter(k_labels).most_common(1)[0][0]

7.  image classification using a pre-trained model (Transfer Learning)

from keras.applications import VGG16
from keras.models import Model
from keras.layers import Dense, Flatten

base_model = VGG16(include_top=False, input_shape=(224, 224, 3))
x = Flatten()(base_model.output)
x = Dense(64, activation='relu')(x)
output = Dense(10, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)
for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

8. k-means clustering algorithm.

import numpy as np

def k_means(X, k, epochs=100):
    centroids = X[np.random.choice(len(X), k, replace=False)]
    for _ in range(epochs):
        labels = [np.argmin([np.linalg.norm(x-c) for c in centroids]) for x in X]
        new_centroids = [np.mean([x for x, label in zip(X, labels) if label == i], axis=0) for i in range(k)]
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    return centroids, labels

9. text classifier using Naive Bayes

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(text_data)
model = MultinomialNB()
model.fit(X, y_labels)

10. Recurrent Neural Network (RNN) for sequence prediction

from keras.models import Sequential
from keras.layers import SimpleRNN, Dense
model = Sequential([SimpleRNN(50, activation='tanh', input_shape=(10, 1)), Dense(1)])
model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=10)